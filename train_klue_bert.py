# -*- coding: utf-8 -*-
"""bert-training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x0cKwOBPqkwCp94PyyfXIxCeZoDsGwU0

**BERT모델을 Fine-tuning하여 Multi-Class Text Classification 수행**

> Klue-Bert 파인튜닝

**CSV to JSON (text, emotion 추출)**
"""

import pandas as pd

file_path = "/content/drive/MyDrive/datasets/감정분석결과(new).csv"
df = pd.read_csv(file_path, encoding='utf-8-sig')

# 필요한 컬럼만
df = df[['text', 'emotion']]

# 감정별 300개 이상인 클래스만 필터링
emotion_counts = df['emotion'].value_counts()
valid_emotions = emotion_counts[emotion_counts >= 300].index.tolist()
df = df[df['emotion'].isin(valid_emotions)]

# 각 감정을 무작위로 300개씩 샘플링
balanced_df = (
    df.groupby('emotion', group_keys=False)
    .apply(lambda x: x.sample(n=300, random_state=42))
    .reset_index(drop=True)
)

# 저장
output_path = "text_emotion_data_300each_filtered.json"
balanced_df.to_json(output_path, orient='records', force_ascii=False, indent=2)
print(f"소수 클래스 제거 후 JSON 저장 완료: {output_path}")

"""**JSON 데이터 확인**"""

import json
import pandas as pd
# -*- coding: utf-8 -*-
# Load Train-set
with open('/content/text_emotion_data_300each_filtered.json', mode='rt', encoding='utf-8-sig') as f:
    train_dataset_raw = json.load(f)

train_dataset_list = [{'text':data['text'], 'label':data['emotion']} for data in train_dataset_raw]
train_df = pd.DataFrame(train_dataset_list)
train_df.head()

# prompt: 각 label에 해당하는 5개의 text를 볼수 있게 하는 코드

import pandas as pd
def display_sample_texts_per_label(df, num_samples=5):
  """
    각 레이블별로 샘플 텍스트를 출력합니다.

    Args:
      df (pd.DataFrame): 'text'와 'label' 컬럼을 포함하는 데이터프레임.
      num_samples (int): 각 레이블별로 출력할 샘플 텍스트의 개수.
    """
  for label in df['label'].unique():
    print(f"Label: {label}")
    print("-" * 20)
    sample_texts = df[df['label'] == label]['text'].sample(min(num_samples, len(df[df['label'] == label])))
    for i, text in enumerate(sample_texts):
      print(f"{i+1}. {text}")
    print("\n")

display_sample_texts_per_label(train_df)

train_df.groupby(by=['label']).count()

"""**감정 라벨 숫자로 인코딩**"""

import numpy as np
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
label_encoder.fit(train_df['label'])
num_labels = len(label_encoder.classes_)

train_df['encoded_label'] = np.asarray(label_encoder.transform(train_df['label']), dtype=np.int32)
train_df.head()

"""**Spliting data into training and validation set**"""

train_texts = train_df["text"].to_list() # Features (not-tokenized yet)
train_labels = train_df["encoded_label"].to_list() # Labels

from sklearn.model_selection import train_test_split

# Split Train and Validation data
#train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=0)
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts,
    train_labels,
    test_size=0.2,
    stratify=train_labels,
    random_state=0
)

"""**Load Tokenizer and Tokenizing**"""

HUGGINGFACE_MODEL_PATH = "klue/bert-base"
from transformers import BertTokenizerFast

# Load Tokenizer
tokenizer = BertTokenizerFast.from_pretrained(HUGGINGFACE_MODEL_PATH,from_pt=True)

# Tokenizing
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)

"""**tokenized 된 데이터 셋을 Tensorflow의 Dataset object로 변환**"""

import tensorflow as tf

# trainset-set
train_dataset = tf.data.Dataset.from_tensor_slices({
    'input_ids': train_encodings['input_ids'],
    'token_type_ids': train_encodings['token_type_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels': train_labels # Add labels with the key 'labels'
})

# validation-set
val_dataset = tf.data.Dataset.from_tensor_slices({
    'input_ids': val_encodings['input_ids'],
    'token_type_ids': val_encodings['token_type_ids'],
    'attention_mask': val_encodings['attention_mask'],
    'labels': val_labels # Add labels with the key 'labels'
})

print(train_dataset)
print(val_dataset)

"""**Fine Tuning Using Native Tensorflow**

"""

from transformers import TFBertForSequenceClassification, create_optimizer

num_labels = len(label_encoder.classes_)
model = TFBertForSequenceClassification.from_pretrained(
    HUGGINGFACE_MODEL_PATH,
    num_labels=num_labels,
    from_pt=True
)

# 옵티마이저 생성
batch_size = 16
epochs = 3
steps_per_epoch = len(train_dataset) // batch_size
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1 * num_train_steps)

optimizer, lr_schedule = create_optimizer(
    init_lr=5e-5,
    num_train_steps=num_train_steps,
    num_warmup_steps=num_warmup_steps
)

import tensorflow as tf
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# GPU 메모리 점진 할당
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

"""**Training**"""

from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import EarlyStopping


early_stop = EarlyStopping(
    monitor='val_loss',
    patience=2,
    restore_best_weights=True,
    mode = 'auto')

model.fit(
    train_dataset.shuffle(500).batch(16),
    epochs=3,
    validation_data=val_dataset.batch(16),
    #callbacks=[early_stop]  # 또는 [callback_earlystop]
)

"""**Change id2label, label2id in model.config**"""

import re

id2labels = model.config.id2label
model.config.id2label = {id : label_encoder.inverse_transform([int(re.sub('LABEL_', '', label))])[0]  for id, label in id2labels.items()}

label2ids = model.config.label2id
model.config.label2id = {label_encoder.inverse_transform([int(re.sub('LABEL_', '', label))])[0] : id   for id, label in id2labels.items()}

"""**Saving the model and tokenizer**"""

import os
MODEL_NAME = 'fine-tuned-klue-bert-basev3'
MODEL_SAVE_PATH = os.path.join("/content/drive/MyDrive/_model", MODEL_NAME) # change this to your preferred location

if os.path.exists(MODEL_SAVE_PATH):
    print(f"{MODEL_SAVE_PATH} -- Folder already exists \n")
else:
    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)
    print(f"{MODEL_SAVE_PATH} -- Folder create complete \n")

# save tokenizer, model
model.save_pretrained(MODEL_SAVE_PATH)
tokenizer.save_pretrained(MODEL_SAVE_PATH)

"""**Usage**"""

import tensorflow as tf
from transformers import BertTokenizerFast, TFBertForSequenceClassification
import numpy as np

MODEL_SAVE_PATH = '/content/drive/MyDrive/_model/fine-tuned-klue-bert-basev3'

tokenizer = BertTokenizerFast.from_pretrained(MODEL_SAVE_PATH)
model = TFBertForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)

# 예측 함수
def classify_text(text):
    inputs = tokenizer(text, return_tensors="tf", truncation=True, padding=True)
    outputs = model(**inputs)
    logits = outputs.logits
    probs = tf.nn.softmax(logits, axis=-1)
    predicted_class = tf.argmax(probs, axis=1).numpy()[0]
    return {
        "label": int(predicted_class),
        "scores": probs.numpy()[0].tolist()
    }

# 라벨 목록 (index 0부터 순서대로)
labels = ["기대감", "기쁨", "놀람", "분노", "불쾌함", "사랑", "슬픔"]
,
# 출력 포맷 함수
def pretty_print_result(result):
    pred_idx = result["label"]
    print(f"예측 감정: {labels[pred_idx]}")
    print("확률 분포:")
    for i, score in enumerate(result["scores"]):
        print(f"  {labels[i]}: {score:.2%}")

# 테스트
sample_text = """오늘은 너무 몸이 가벼운 하루. 아침에 일어났더니 너무 상쾌했고, 해야할 과제도 어제 다 끝내고 기분 최상이었다. 점심도 맛있었고, 저녁은 끝내주게 좋았다."""
result = classify_text(sample_text)
pretty_print_result(result)

"""**Evaluation**"""

from transformers import TextClassificationPipeline

# Load Fine-tuning model
loaded_tokenizer = BertTokenizerFast.from_pretrained(MODEL_SAVE_PATH)
loaded_model = TFBertForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)

text_classifier = TextClassificationPipeline(
    tokenizer=loaded_tokenizer,
    model=loaded_model,
    framework='tf',
    return_all_scores=True
)

# Load Test-set
with open('/content/drive/MyDrive/datasets/test_text_emotion_data.json', mode='rt', encoding='utf-8-sig') as f:
    test_dataset = json.load(f)

test_dataset_list = [{'text':data['text'], 'label':data['emotion']} for data in test_dataset]
test_df = pd.DataFrame(test_dataset_list)
test_df.head()
#print(len(test_df))

predicted_label_list = []
predicted_score_list = []

for text in test_df['text']:
    # predict
    preds_list = text_classifier(text)[0]

    sorted_preds_list = sorted(preds_list, key=lambda x: x['score'], reverse=True)
    predicted_label_list.append(sorted_preds_list[0]) # label
    predicted_score_list.append(sorted_preds_list[1]) # score

test_df['pred'] = predicted_label_list
test_df['score'] = predicted_score_list
test_df.head()

test_df['pred_label'] = test_df['pred'].apply(lambda x: x['label'])

# classification_report 사용
from sklearn.metrics import classification_report
print(classification_report(y_true=test_df['label'], y_pred=test_df['pred_label']))